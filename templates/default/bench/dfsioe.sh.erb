#!/bin/bash

trap "" HUP

source <%=node['hops']['conf_dir']%>/hadoop-env.sh 

export PATH=<%=node['hops']['bin_dir']%>:$PATH

DFSIO_JAR=<%= @dfsio_jar%>

FILES=$1
#FileSize in MB
FILESIZE=$2

DATE=`date +%Y-%m-%d-%H-%M-%S`

LOGDIR=logs/dfsioe/$FILESIZE/$DATE

if [ ! -d "$LOGDIR" ]
then
    mkdir -p ./$LOGDIR
fi

INPUT_HDFS="/dfsioe-test"
WRITE_OPTION="-write -nrFiles ${FILES} -fileSize ${FILESIZE} -bufferSize 4096 -plotInteval 1000 -sampleUnit m -sampleInteval 200 -sumThreshold 0.5 -tputReportTotal"
READ_OPTION="-read -nrFiles ${FILES} -fileSize ${FILESIZE} -bufferSize 131072 -plotInteval 1000 -sampleUnit m -sampleInteval 200 -sumThreshold 0.5 -tputReportTotal"

OLD_HADOOP_OPTS=${HADOOP_OPTS:-}
export HADOOP_OPTS="${HADOOP_OPTS:-} -Dtest.build.data=${INPUT_HDFS} "

DFSIO_WRITE_OUTPUT_FILE="./$LOGDIR/dfsioe_write_results.txt"
DFSIO_WRITE_THROUGHPUT_OUTPUT_FILE="./$LOGDIR/dfsioe_write_throughput_results.csv"
DFSIO_WRITE_LOG_OUTPUT_FILE="./$LOGDIR/dfsioe_write_log.txt"

DFSIO_READ_OUTPUT_FILE="./$LOGDIR/dfsioe_read_results.txt"
DFSIO_READ_THROUGHPUT_OUTPUT_FILE="./$LOGDIR/dfsioe_read_throughput_results.txt"
DFSIO_READ_LOG_OUTPUT_FILE="./$LOGDIR/dfsioe_read_log.txt"

echo Running DFSIOe clean job
echo ===============================================================
yarn jar $DFSIO_JAR TestDFSIOEnh  -clean -Dtest.build.data=${INPUT_HDFS}

./run_nmon.sh

echo Running DFSIOe WRITE job
echo ===============================================================
{ yarn jar $DFSIO_JAR TestDFSIOEnh   \
-Dtest.build.data=${INPUT_HDFS} \
-Dmapreduce.map.java.opts="-Dtest.build.data=${INPUT_HDFS} -Xmx1229m"    \
-Dmapreduce.reduce.java.opts="-Dtest.build.data=${INPUT_HDFS} -Xmx2458m" \
-Dmapreduce.map.cpu.vcores=1 \
-Dmapreduce.map.memory.mb=1536 \
-Dmapreduce.reduce.cpu.vcores=1 \
-Dmapreduce.reduce.memory.mb=3072 \
-Dmapreduce.task.io.sort.mb=200 \
-Dmapreduce.task.io.sort.factor=48 \
-Dmapred.tasktracker.map.tasks.maximum=16 \
-Dmapred.tasktracker.reduce.tasks.maximum=8 \
${WRITE_OPTION} -resFile ${DFSIO_WRITE_OUTPUT_FILE}  \
-tputFile ${DFSIO_WRITE_THROUGHPUT_OUTPUT_FILE} ;} > ${DFSIO_WRITE_LOG_OUTPUT_FILE} 2>&1

NMON_DIR=./$LOGDIR/nmon-write
mkdir -p $NMON_DIR

./stop_and_collect_nmon.sh ${NMON_DIR}

./run_nmon.sh

echo Running DFSIOe READ job
echo ===============================================================
{ yarn jar $DFSIO_JAR TestDFSIOEnh   \
-Dtest.build.data=${INPUT_HDFS} \
-Dmapreduce.map.java.opts="-Dtest.build.data=${INPUT_HDFS} -Xmx1229m"    \
-Dmapreduce.reduce.java.opts="-Dtest.build.data=${INPUT_HDFS} -Xmx2458m" \
-Dmapreduce.map.cpu.vcores=1 \
-Dmapreduce.map.memory.mb=1536 \
-Dmapreduce.reduce.cpu.vcores=1 \
-Dmapreduce.reduce.memory.mb=3072 \
-Dmapreduce.task.io.sort.mb=200 \
-Dmapreduce.task.io.sort.factor=48 \
-Dmapred.tasktracker.map.tasks.maximum=16 \
-Dmapred.tasktracker.reduce.tasks.maximum=8 \
${READ_OPTION} -resFile ${DFSIO_READ_OUTPUT_FILE}  \
-tputFile ${DFSIO_READ_THROUGHPUT_OUTPUT_FILE} ; } > ${DFSIO_READ_LOG_OUTPUT_FILE} 2>&1

NMON_DIR=./$LOGDIR/nmon-read
mkdir -p $NMON_DIR

./stop_and_collect_nmon.sh ${NMON_DIR}

export HADOOP_OPTS="$OLD_HADOOP_OPTS"