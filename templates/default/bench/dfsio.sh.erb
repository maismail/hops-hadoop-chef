#!/bin/bash

trap "" HUP

#if [ $EUID -eq 0 ]; then
#   echo "this script must not be run as root. su to hdfs user to run"
#   exit 1
#fi

source <%=node['hops']['conf_dir']%>/hadoop-env.sh 

export PATH=<%=node['hops']['bin_dir']%>:$PATH

DFSIO_JAR=<%= @dfsio_jar%>
FILES=20
FILESIZE=1GB

LOGDIR=logs

if [ ! -d "$LOGDIR" ]
then
    mkdir ./$LOGDIR
fi

DATE=`date +%Y-%m-%d:%H:%M:%S`


DFSIO_WRITE_OUTPUT_FILE="./$LOGDIR/dfsio_write_results.txt_$DATE"

DFSIO_READ_OUTPUT_FILE="./$LOGDIR/dfsio_read_results.txt_$DATE"



echo Running DFSIO CLEAN job
echo ===============================================================
yarn jar $DFSIO_JAR TestDFSIO \
-Dmapreduce.map.output.compress=true \
-Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.Lz4Codec \
-clean


echo Running DFSIO WRITE job
echo ===============================================================
yarn jar $DFSIO_JAR TestDFSIO \
-Dmapreduce.map.output.compress=true \
-Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.Lz4Codec \
-write -nrFiles $FILES \
-fileSize $FILESIZE \
-resFile $DFSIO_WRITE_OUTPUT_FILE



echo Running DFSIO READ job
echo ===============================================================
yarn jar $DFSIO_JAR TestDFSIO \
-Dmapreduce.map.output.compress=true \
-Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.Lz4Codec \
-read -nrFiles $FILES \
-fileSize $FILESIZE \
-resFile $DFSIO_READ_OUTPUT_FILE
